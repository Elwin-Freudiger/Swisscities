{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Swiss cities based on Aerial Imagery\n",
    "\n",
    "the full code related to this project can be found on github: https://github.com/Elwin-Freudiger/Swisscities\n",
    "\n",
    "Aerial imagery is used in a lot of fields. This project attempts to see how features can be extracted from images and used to classify cities in Switzerland.\n",
    "\n",
    "All of the images used in this project were obtained thanks to the *Federal Office of Topography swisstopo*, their website allows anyone to download a mosaic of aerial images. These images are taken all across Switzerland and the edge of the country. Each image has a ground resolution of 10cm, except over the Alps where the ground resolution is 25cm. These tiles are $1km^2$, For this research, a lower download resolution of 2 meters was chosen.\n",
    "\n",
    "the link to download the data is: https://www.swisstopo.admin.ch/en/orthoimage-swissimage-10\n",
    "\n",
    "## Feature extraction:\n",
    "\n",
    "Based on an image, features can be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageShow\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx\n",
    "import xyzservices.providers as xyz\n",
    "from alive_progress import alive_bar; import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open('report/sample2.tiff') as file:\n",
    "        img_array = file.read([1, 2, 3]).transpose((1, 2, 0))  # Correct dimension order for OpenCV\n",
    "        metadata = file.meta\n",
    "        corner = metadata['transform']\n",
    "        center_x, center_y = corner * (metadata['width'] // 2, metadata['height'] // 2)\n",
    "\n",
    "ImageShow(img_array)\n",
    "mean = np.mean(img_array)\n",
    "print(mean)\n",
    "stdev = np.std(img_array)\n",
    "print(stdev)\n",
    "\n",
    "gray = cv.cvtColor(img_array, cv.COLOR_RGB2GRAY)\n",
    "gray_dist = cv.calcHist([gray], [0], None, [256], [0,256])                    \n",
    "red = cv.calcHist([img_array], [0], None, [256], [0, 256])\n",
    "green = cv.calcHist([img_array], [1], None, [256], [0, 256])\n",
    "blue = cv.calcHist([img_array], [2], None, [256], [0, 256])\n",
    "distribution = np.column_stack((gray_dist.flatten(), red.flatten(), green.flatten(), blue.flatten()))\n",
    "fig, axs = plt.subplots(2,2)\n",
    "axs[0,0].plot(gray_dist, color='gray')\n",
    "axs[0,1].plot(red, color = 'red')\n",
    "axs[1,0].plot(green, color = 'green')\n",
    "axs[1,1].plot(blue, color='blue')\n",
    "axs[0,0].set_title('Grayscale distribution')\n",
    "axs[0,1].set_title('Red distribution')\n",
    "axs[1,0].set_title('Green distribution')\n",
    "axs[1,1].set_title('Blue distribution')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "co_matrix = skimage.feature.graycomatrix(gray, [5], [0], levels=256, normed=True)\n",
    "contrast = skimage.feature.graycoprops(co_matrix, 'contrast')[0, 0]\n",
    "print(contrast)\n",
    "correlation = skimage.feature.graycoprops(co_matrix, 'correlation')[0, 0]\n",
    "print(correlation)\n",
    "energy = skimage.feature.graycoprops(co_matrix, 'energy')[0, 0]\n",
    "print(energy)\n",
    "homogeneity = skimage.feature.graycoprops(co_matrix, 'homogeneity')[0, 0]\n",
    "print(homogeneity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the images\n",
    "\n",
    "First, we will look at how the images are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "City_list = ['Zurich', 'Geneva', 'Basel', 'Lausanne', 'Bern', 'Winterthur', 'Luzern', 'St_Gallen', 'Lugano', 'Biel']\n",
    "Balanced_list = ['Bern', 'Zurich', 'Lugano', 'Lausanne', 'Chur', 'Schwyz', 'Glarus', 'Winterthur', 'Sarnen', 'Nendaz']\n",
    "\n",
    "def extraction(link, city):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with rasterio.io.MemoryFile(response.content) as file:\n",
    "            with file.open() as dataset:\n",
    "                img_array = dataset.read([1, 2, 3]).transpose((1, 2, 0))  # Correct dimension order for OpenCV\n",
    "                metadata = dataset.meta\n",
    "                corner = metadata['transform']\n",
    "                center_x, center_y = corner * (metadata['width'] // 2, metadata['height'] // 2)\n",
    "\n",
    "                mean = np.mean(img_array)\n",
    "                stdev = np.std(img_array)\n",
    "\n",
    "                gray = cv.cvtColor(img_array, cv.COLOR_RGB2GRAY)\n",
    "                gray_dist = cv.calcHist([gray], [0], None, [256], [0,256])                    \n",
    "                red = cv.calcHist([img_array], [0], None, [256], [0, 256])\n",
    "                green = cv.calcHist([img_array], [1], None, [256], [0, 256])\n",
    "                blue = cv.calcHist([img_array], [2], None, [256], [0, 256])\n",
    "                distribution = np.column_stack((gray_dist.flatten(), red.flatten(), green.flatten(), blue.flatten()))\n",
    "\n",
    "                co_matrix = skimage.feature.graycomatrix(gray, [5], [0], levels=256, normed=True)\n",
    "                contrast = skimage.feature.graycoprops(co_matrix, 'contrast')[0, 0]\n",
    "                correlation = skimage.feature.graycoprops(co_matrix, 'correlation')[0, 0]\n",
    "                energy = skimage.feature.graycoprops(co_matrix, 'energy')[0, 0]\n",
    "                homogeneity = skimage.feature.graycoprops(co_matrix, 'homogeneity')[0, 0]\n",
    "                \n",
    "                row = [city, center_x, center_y, mean, stdev, distribution.tolist(), contrast, correlation, energy, homogeneity]\n",
    "                return row\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {link}: {e}\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "def main(list):\n",
    "    data = []\n",
    "    for city in list:\n",
    "        path = f'data/city_link/{city}.csv'\n",
    "        with open(path, 'r') as link_list:\n",
    "            links = link_list.read().splitlines()\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            with alive_bar(len(links)) as bar:\n",
    "                future_images = {executor.submit(extraction, link, city): link for link in links}\n",
    "                for future in as_completed(future_images):\n",
    "                    row = future.result()\n",
    "                    if row is not None:\n",
    "                        data.append(row)\n",
    "                    bar()\n",
    "                    \n",
    "    return pd.DataFrame(data, columns=['Location', 'East', 'North', 'Mean', 'Stdev', 'Distribution', 'Contrast', 'Correlation', 'Energy', 'Homogeneity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top10 = main(City_list)\n",
    "df_balanced = main(Balanced_list) \n",
    "df_top10.head()\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df):\n",
    "    crs_2056 = \"EPSG:2056\"\n",
    "\n",
    "    path = 'report/swissBOUNDARIES3D_1_5_LV95_LN02.gdb'\n",
    "    swiss_borders = gpd.read_file(path, layer='TLM_HOHEITSGRENZE')\n",
    "    canton_border = swiss_borders[swiss_borders['OBJEKTART'].isin([0])]\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(df['East'], df['North'])]\n",
    "\n",
    "    gdf_points = gpd.GeoDataFrame(df, geometry=geometry, crs=crs_2056)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    canton_border['geometry'].plot(ax=ax, color='grey', alpha=0)\n",
    "    gdf_points.plot(ax=ax, color='red', markersize=1)\n",
    "\n",
    "    swiss_basemap = xyz.SwissFederalGeoportal.NationalMapColor\n",
    "    ctx.add_basemap(ax, crs=crs_2056, source=swiss_basemap)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df_top10)\n",
    "print(df_top10.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df_balanced)\n",
    "print(df_balanced.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ['temperature', 'sunshine', 'rainfall']\n",
    "\n",
    "def main(df, name):\n",
    "    for file in info:\n",
    "        path = f'data/{file}.tiff'\n",
    "        with rasterio.open(path) as dataset:\n",
    "            meta = dataset.meta\n",
    "            matrix = dataset.read(1)\n",
    "            func = meta['transform']\n",
    "\n",
    "            def get_value(est, nord):\n",
    "                x, y = ~func * (est, nord)\n",
    "                x = floor(x)\n",
    "                y = floor(y)\n",
    "                return matrix[y, x]\n",
    "\n",
    "        get_value_vector = np.vectorize(get_value)\n",
    "\n",
    "        df[file] = get_value_vector(df['East'], df['North'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top10_withinfo = main(df_top10, 'top10')\n",
    "df_top10_withinfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_withinfo = main(df_balanced, 'balanced')\n",
    "df_balanced_withinfo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras API:\n",
    "\n",
    "The API can help create great deep learning models\n",
    "\n",
    "https://keras.io/guides/functional_api/\n",
    "\n",
    "Our model will have mutiple inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, utils\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Dropout, Dense, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df):\n",
    "    df['Location'] = pd.Categorical(df['Location'])\n",
    "    df['Location_code'] = df['Location'].cat.codes\n",
    "    city_names = df['Location'].cat.categories\n",
    "\n",
    "    numeric_features = ['Mean', 'Stdev', 'Contrast', 'Correlation', 'Energy', 'Homogeneity', 'temperature', 'sunshine', 'rainfall']\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "    Numeric_train, Numeric_test = train_test_split(df[numeric_features], test_size=0.3, random_state=42)\n",
    "    y_train, y_test = train_test_split(df['Location_code'], test_size=0.3, random_state=42)\n",
    "    Image_string_train, Image_string_test = train_test_split(df[['Distribution']], test_size=0.3, random_state=42)\n",
    "\n",
    "    y_train = np.array(y_train.to_list())\n",
    "    y_test = np.array(y_test.to_list())\n",
    "\n",
    "    Numeric_train = Numeric_train.to_numpy()\n",
    "    Numeric_test = Numeric_test.to_numpy()\n",
    "\n",
    "    def scale_each_channel(image_str):\n",
    "        array = np.array(eval(image_str)) \n",
    "        scaled_array = np.zeros_like(array, dtype=np.float32)\n",
    "        for channel in range(array.shape[1]):\n",
    "            channel_data = array[:, channel]\n",
    "            scaled_array[:, channel] = (channel_data - np.min(channel_data)) / (np.max(channel_data) - np.min(channel_data))\n",
    "        return scaled_array\n",
    "\n",
    "    Image_train = []\n",
    "    for i in Image_string_train['Distribution'].to_numpy():\n",
    "        scaled_array = scale_each_channel(i)\n",
    "        Image_train.append(scaled_array)\n",
    "    Image_train = np.array(Image_train)\n",
    "\n",
    "    Image_test = []\n",
    "    for i in Image_string_test['Distribution'].to_numpy():\n",
    "        scaled_array = scale_each_channel(i)\n",
    "        Image_test.append(scaled_array)\n",
    "    Image_test = np.array(Image_test)\n",
    "\n",
    "    numeric_input = Input(shape=(9,), name='Numeric')\n",
    "    image_input = Input(shape=(256, 4), name='Image')\n",
    "\n",
    "    numeric_dense = Dense(64, activation='relu')(numeric_input)\n",
    "\n",
    "    image_flatten = Flatten()(image_input)\n",
    "    image_dense = Dense(128, activation='relu')(image_flatten)\n",
    "    image_drop = Dropout(0.5)(image_dense)\n",
    "    image_dense = Dense(128, activation='relu')(image_drop)\n",
    "\n",
    "    x = layers.concatenate([numeric_dense, image_dense])\n",
    "\n",
    "    xdense = Dense(64, activation='relu')(x)\n",
    "    output = Dense(10, activation='softmax')(xdense)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, image_input], outputs=output)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.fit(\n",
    "        {\"Numeric\": Numeric_train, \"Image\": Image_train},\n",
    "        y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate([Numeric_test, Image_test], y_test, verbose=2)\n",
    "    print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "    y_pred_probs = model.predict([Numeric_test, Image_test])\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=city_names, yticklabels=city_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(df_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(df_top10_withinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(df_balanced_withinfo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
